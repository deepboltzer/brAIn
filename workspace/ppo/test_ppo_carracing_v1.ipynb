{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope.\" - [CarRacing-v0](https://gym.openai.com/envs/CarRacing-v0/) ([Source](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent for 500000 timesteps\n",
    "for i in range(1,501):\n",
    "\n",
    "    iterations = i*1000\n",
    "\n",
    "    # Train the agent for 1000 steps at a time\n",
    "    model.learn(total_timesteps=1000)\n",
    "\n",
    "    print(f\"\\n-----\\nProgress:{iterations}/500000\\n-----\\n\")\n",
    "\n",
    "    if iterations % 5000 == 0:\n",
    "        # Save model\n",
    "        save_dir = \"./models/\"\n",
    "        model.save(save_dir + f\"/carracing-v1/carracing_v1_ppo_{iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1284..1609 -> 325-tiles track\n",
      "Track generation: 1100..1373 -> 273-tiles track\n",
      "Track generation: 1127..1413 -> 286-tiles track\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Track generation: 1029..1292 -> 263-tiles track\n",
      "Track generation: 1278..1602 -> 324-tiles track\n",
      "Track generation: 1131..1418 -> 287-tiles track\n",
      "Track generation: 1090..1371 -> 281-tiles track\n",
      "Track generation: 1187..1488 -> 301-tiles track\n",
      "Track generation: 1107..1388 -> 281-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 1110..1392 -> 282-tiles track\n",
      "Track generation: 1030..1294 -> 264-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1026..1295 -> 269-tiles track\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Track generation: 1032..1294 -> 262-tiles track\n",
      "Track generation: 1007..1271 -> 264-tiles track\n",
      "Track generation: 1238..1551 -> 313-tiles track\n",
      "Track generation: 966..1212 -> 246-tiles track\n",
      "Track generation: 1268..1589 -> 321-tiles track\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "Track generation: 1191..1493 -> 302-tiles track\n",
      "Track generation: 1137..1435 -> 298-tiles track\n",
      "Track generation: 1278..1602 -> 324-tiles track\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "Track generation: 1259..1584 -> 325-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1142..1438 -> 296-tiles track\n",
      "Track generation: 1117..1402 -> 285-tiles track\n",
      "Track generation: 1023..1283 -> 260-tiles track\n",
      "Track generation: 1038..1304 -> 266-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1096..1382 -> 286-tiles track\n",
      "Track generation: 1089..1365 -> 276-tiles track\n",
      "Track generation: 1099..1377 -> 278-tiles track\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Track generation: 1028..1296 -> 268-tiles track\n",
      "Track generation: 1256..1581 -> 325-tiles track\n",
      "Track generation: 1248..1564 -> 316-tiles track\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Track generation: 1101..1380 -> 279-tiles track\n",
      "Track generation: 1256..1574 -> 318-tiles track\n",
      "Track generation: 1144..1442 -> 298-tiles track\n",
      "Track generation: 1114..1397 -> 283-tiles track\n",
      "Track generation: 1307..1638 -> 331-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 1035..1298 -> 263-tiles track\n",
      "Track generation: 1092..1379 -> 287-tiles track\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "mean_reward (agent trained over 500000 timesteps): -87.82 +/- 42.38\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "save_dir = \"./models/\"\n",
    "del model\n",
    "model = PPO.load(save_dir + f\"/carracing-v1/carracing_v1_ppo_500000\")\n",
    "\n",
    "# Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=50)\n",
    "\n",
    "print(f\"mean_reward (agent trained over 500000 timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'timesteps': [], 'mean_reward': [], 'std_reward': []}\n",
    "\n",
    "for i in range(1,501):\n",
    "\n",
    "    timesteps = i*1000\n",
    "\n",
    "    if timesteps % 5000 == 0:\n",
    "        # Load model\n",
    "        del model\n",
    "        model = PPO.load(save_dir + f\"/carracing-v1/carracing_v1_ppo_{timesteps}\")    \n",
    "        \n",
    "        # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "        mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "        \n",
    "        print(f\"mean_reward (agent trained over {timesteps} timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        \n",
    "        data['timesteps'].append(timesteps)\n",
    "        data['mean_reward'].append(mean_reward)\n",
    "        data['std_reward'].append(std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into pandas.DataFrame and serialize as csv file\n",
    "df = pd.DataFrame(data, columns=data.keys())\n",
    "df.set_index('timesteps', inplace=True)\n",
    "\n",
    "df.to_csv(f\"./out/training-log/carracing_v1_ppo_{df.index.to_list()[-1]}.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
