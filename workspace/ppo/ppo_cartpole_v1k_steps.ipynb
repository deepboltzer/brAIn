{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\" [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/), \n",
    "\n",
    "<s>[Max episode length is 500](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), therefore the max reward is also 500.</s>\n",
    "\n",
    "This is a custom version of CartPole that has a maximum for episode steps of 1000.\n",
    "\n",
    "[Registering custom environments](https://stackoverflow.com/questions/42787924/why-is-episode-done-after-200-time-steps-gym-environment-mountaincar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to re-run unless output is cleared; registration is saved in PATH\\TO\\ENV\\lib\\site-packages\\gym\\envs\\registration.py\n",
    "# Re-running will throw UserWarning: PATH\\TO\\ENV\\lib\\site-packages\\gym\\envs\\registration.py:595: \n",
    "# UserWarning: WARN: Overriding environment CartPole-1k\n",
    "gym.envs.register(\n",
    "    id=\"CartPole-v1k\",\n",
    "    entry_point=\"gym.envs.classic_control:CartPoleEnv\",\n",
    "    max_episode_steps=1000, # CartPole-v1 uses 500\n",
    "    reward_threshold=975.0, # CartPole-v1 uses 475.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1k')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,31):\n",
    "\n",
    "    iterations = i*500\n",
    "\n",
    "    # Train the agent for 500 steps\n",
    "    model.learn(total_timesteps=500) \n",
    "\n",
    "    # Save model\n",
    "    save_dir = \"./models/\"\n",
    "    model.save(save_dir + f\"/cartpole-v1k/cartpole_v1k_ppo_{iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward (agent trained over 500 timesteps): 113.46 +/- 84.18\n",
      "mean_reward (agent trained over 1000 timesteps): 280.72 +/- 190.36\n",
      "mean_reward (agent trained over 1500 timesteps): 282.94 +/- 222.04\n",
      "mean_reward (agent trained over 2000 timesteps): 245.23 +/- 168.19\n",
      "mean_reward (agent trained over 2500 timesteps): 377.14 +/- 197.35\n",
      "mean_reward (agent trained over 3000 timesteps): 472.94 +/- 232.75\n",
      "mean_reward (agent trained over 3500 timesteps): 509.95 +/- 231.40\n",
      "mean_reward (agent trained over 4000 timesteps): 700.82 +/- 254.36\n",
      "mean_reward (agent trained over 4500 timesteps): 492.70 +/- 151.78\n",
      "mean_reward (agent trained over 5000 timesteps): 793.59 +/- 246.56\n",
      "mean_reward (agent trained over 5500 timesteps): 869.34 +/- 194.30\n",
      "mean_reward (agent trained over 6000 timesteps): 881.09 +/- 185.32\n",
      "mean_reward (agent trained over 6500 timesteps): 922.75 +/- 131.46\n",
      "mean_reward (agent trained over 7000 timesteps): 915.06 +/- 155.20\n",
      "mean_reward (agent trained over 7500 timesteps): 870.82 +/- 168.61\n",
      "mean_reward (agent trained over 8000 timesteps): 965.18 +/- 92.18\n",
      "mean_reward (agent trained over 8500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 9000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 9500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 10000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 10500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 11000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 11500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 12000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 12500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 13000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 13500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 14000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 14500 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 15000 timesteps): 1000.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "data ={'timesteps': [], 'mean_reward': [], 'std_reward': []}\n",
    "\n",
    "for i in range(1,31):\n",
    "\n",
    "    timesteps = i*500\n",
    "\n",
    "    # Load model\n",
    "    del model\n",
    "    model = PPO.load(save_dir + f\"/cartpole-v1k/cartpole_v1k_ppo_{timesteps}\")\n",
    "\n",
    "    # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "    mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "    print(f\"mean_reward (agent trained over {timesteps} timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    data['timesteps'].append(timesteps)\n",
    "    data['mean_reward'].append(mean_reward)\n",
    "    data['std_reward'].append(std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into pandas.DataFrame and serialize as csv file\n",
    "df = pd.DataFrame(data, columns=data.keys())\n",
    "df.set_index('timesteps', inplace=True)\n",
    "\n",
    "df.to_csv(f\"./out/cartpole-v1k/cartpole_v1k_ppo_{df.index.to_list()[-1]}.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
