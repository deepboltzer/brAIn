{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\" [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/), \n",
    "\n",
    "[Max episode length is 500](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), therefore the max reward is also 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent for 15000 timesteps\n",
    "for i in range(1,31):\n",
    "\n",
    "    iterations = i*500\n",
    "\n",
    "    # Train the agent for 500 steps at a time\n",
    "    model.learn(total_timesteps=500)\n",
    "\n",
    "    # Save model\n",
    "    save_dir = \"./models/\"\n",
    "    model.save(save_dir + f\"/cartpole-v1/cartpole_v1_ppo_{iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'timesteps': [], 'mean_reward': [], 'std_reward': []}\n",
    "\n",
    "for i in range(1,31):\n",
    "\n",
    "    timesteps = i*500\n",
    "\n",
    "    # Load model\n",
    "    del model\n",
    "    model = PPO.load(save_dir + f\"/cartpole-v1/cartpole_v1_ppo_{timesteps}\")    \n",
    "    \n",
    "    # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "    mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "    \n",
    "    # print(f\"mean_reward (agent trained over {timesteps} timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    data['timesteps'].append(timesteps)\n",
    "    data['mean_reward'].append(mean_reward)\n",
    "    data['std_reward'].append(std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into pandas.DataFrame and serialize as csv file\n",
    "df = pd.DataFrame(data, columns=data.keys())\n",
    "df.set_index('timesteps', inplace=True)\n",
    "\n",
    "df.to_csv(f\"./out/training-log/cartpole_v1_ppo_{df.index.to_list()[-1]}.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbc3a48a388f122354c0894eb150687a9e10e23f040bc7de2f7a61dc15016a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('adv-attacks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
