{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope.\" - [CarRacing-v0](https://gym.openai.com/envs/CarRacing-v0/) ([Source](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent for 1500 timesteps\n",
    "for i in range(1,31):\n",
    "\n",
    "    iterations = i*500\n",
    "\n",
    "    # Train the agent for 500 steps at a time\n",
    "    model.learn(total_timesteps=500)\n",
    "\n",
    "    # Save model\n",
    "    save_dir = \"./models/\"\n",
    "    model.save(save_dir + f\"/carracing-v1/carracing_v1_ppo_{iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1224..1534 -> 310-tiles track\n",
      "Track generation: 1144..1434 -> 290-tiles track\n",
      "Track generation: 1136..1433 -> 297-tiles track\n",
      "Track generation: 1271..1593 -> 322-tiles track\n",
      "Track generation: 1101..1388 -> 287-tiles track\n",
      "Track generation: 1182..1481 -> 299-tiles track\n",
      "Track generation: 1064..1337 -> 273-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Track generation: 1165..1461 -> 296-tiles track\n",
      "Track generation: 1098..1376 -> 278-tiles track\n",
      "Track generation: 1053..1323 -> 270-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "Track generation: 1128..1421 -> 293-tiles track\n",
      "mean_reward (agent trained over 7500 timesteps): -69.43 +/- 1.21\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "save_dir = \"./models/\"\n",
    "del model\n",
    "model = PPO.load(save_dir + f\"/carracing-v1/carracing_v1_ppo_15000\")\n",
    "\n",
    "# Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=10)\n",
    "\n",
    "print(f\"mean_reward (agent trained over {7500} timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'timesteps': [], 'mean_reward': [], 'std_reward': []}\n",
    "\n",
    "for i in range(1,31):\n",
    "\n",
    "    timesteps = i*500\n",
    "\n",
    "    # Load model\n",
    "    del model\n",
    "    model = PPO.load(save_dir + f\"/carracing-v1/carracing_v1_ppo_{timesteps}\")    \n",
    "    \n",
    "    # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "    mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "    \n",
    "    print(f\"mean_reward (agent trained over {timesteps} timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    data['timesteps'].append(timesteps)\n",
    "    data['mean_reward'].append(mean_reward)\n",
    "    data['std_reward'].append(std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into pandas.DataFrame and serialize as csv file\n",
    "df = pd.DataFrame(data, columns=data.keys())\n",
    "df.set_index('timesteps', inplace=True)\n",
    "\n",
    "df.to_csv(f\"./out/training-log/carracing_v1_ppo_{df.index.to_list()[-1]}.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
