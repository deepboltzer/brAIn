{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\" [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/), \n",
    "\n",
    "[Max episode length is 500](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), therefore the max reward is also 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward (agent trained over 5'000 timesteps): 290.61 +/- 101.96\n",
      "mean_reward (agent trained over 10'000 timesteps): 490.51 +/- 30.90\n",
      "mean_reward (agent trained over 15'000 timesteps): 500.00 +/- 0.00\n",
      "mean_reward (agent trained over 20'000 timesteps): 500.00 +/- 0.00\n",
      "mean_reward (agent trained over 25'000 timesteps): 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,26):\n",
    "\n",
    "    # Train the agent for 1'000 steps\n",
    "    model.learn(total_timesteps=1000) \n",
    "\n",
    "    if i % 5 == 0:\n",
    "\n",
    "        # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "        mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "        print(f\"mean_reward (agent trained over {i}'000 timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "        # Save model\n",
    "        save_dir = \"./models/\"\n",
    "        model.save(save_dir + f\"/cartpole-v1/cartpole_v1_ppo_{i}K\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccbc3a48a388f122354c0894eb150687a9e10e23f040bc7de2f7a61dc15016a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('adv-attacks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
