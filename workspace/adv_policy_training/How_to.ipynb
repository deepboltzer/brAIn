{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step für adversarial-policies github\n",
    "\n",
    "Das Berkley [GitHub](https://github.com/HumanCompatibleAI/adversarial-policies).\n",
    "\n",
    "- Installiere Docker nach Anleitung [hier](https://docs.docker.com/get-docker/)\n",
    "\n",
    "    - Für Windows muss zuvor WSL (Windows Subsystem for Linux) installiert werden. Anleitung davon gibt es bei [Microsoft](https://docs.microsoft.com/en-us/windows/wsl/install)\n",
    "    \n",
    "    - Bei dem ersten Starten von der Desktop Docker Applikation gibt es ein Tutorial welches ganz gut ist.\n",
    "    \n",
    "- Lade das Berkley [GitHub](https://github.com/HumanCompatibleAI/adversarial-policies) herunter.\n",
    "\n",
    "- Lade von der Mujoco [Website](https://www.roboti.us/license.html) den Activation Key herunter. Dies sollte eine Datei mit Namen mjkey.txt sein.\n",
    "\n",
    "- Verschiebe die mjkey.txt in das heruntergeladene Github Repo.\n",
    "\n",
    "- Rufe ein Terminal für Docker auf und navigiere in das Github Repo.\n",
    "\n",
    "- Baue das Docker Image mit ```docker build -t rl_adversarial```. Das image sollte rl_adversarial heißen und im  Docker Desktop auftauchen. (Nach belieben kann man den Namen anders wählen.)\n",
    "\n",
    "- Starte einen Docker Container mit dem Mujoco key durch ```docker run -it --name rl_adv --env MUJOCO_PY_MJKEY_PATH=/adversarial-policies/mjkey.txt rl_adversarial /bin/bash```. nach diesem Aufruf sollte eine Linux Kommandozeile zur  Verfügung stehen wo man nun zB ```python -m aprl.train``` benutzen kann um einen Algo zu trainieren. (rl_adv ist hier der Name des Docker Containers und kann beliebig angepasst werden. Diesr Container sollte auch im Docker Desktop sichtbar sein.) \n",
    "\n",
    "- Mit ```exit```kann der Docker Container gestoppt werden. Um nicht endlos neue Container zu erstellen kann der vorher erstellte Docker Container mittels ```docker start -i  rl_adv``` wieder gestartet werden.\n",
    "\n",
    "Soweit hatte ich mich bis jetzt durchprobiert. Nun sollte man eigentlich im Container mit dem Repo rumspielen können.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 2\n",
    "\n",
    "# HowToTrain\n",
    "\n",
    "In normaler Komandozeile ```docker start -i rl_adv ```\n",
    "\n",
    "\n",
    "Wenn noch nicht trainiert wurde einmal ``` python -m aprl.train ``` ausführen. Das erleichtert später die Suche nach den trainierten models.\n",
    "\n",
    "Sonst als Beispiel für SumoHumans mit der paper config des orginal Experiments(20.000.000 Timesteps) ``` python -m aprl.train with env_name=multicomp/SumoHumans-v0 paper``` \n",
    "\n",
    "Der speichert die erstellten Models in der VM ab. Bei WSL kann man aber einfach den explorer nutzen um diese zu finden(weiter unten)\n",
    "\n",
    "# HowToScore\n",
    "\n",
    "Beispielsweise für SumoHumans: ``` python -m aprl.score_agent with env_name=multicomp/SumoHumans-v0 agent_b_type=ppo2 agent_b_path=/adversarial-policies/data/baselines/20220322_162856-default/final_model/ videos=True ``` \n",
    "\n",
    "Außerdem musste ich in der  ```score_agent.py ``` bei  ```video_params \"annotated\"  ``` auf  ``` False ``` setzten um Videos zu erstellen.\n",
    "\n",
    "Wobei ```20220322_162856-default ``` variieren kann(Für den richtigen PATH: Speicherort). \n",
    "\n",
    "\n",
    "\n",
    "# Speicherort\n",
    "\n",
    "Die Dateien liegen in ``` \\\\wsl$ ``` -> ```docker-desktop-data``` -> ``` version-pack-data ``` -> ``` community``` -> ```docker```-> ```overlay2``` -> Bei mir heißt der Order ``` dea9d59ec2d5e9789a7982db084a3e89ba1a59a7923e0b98c8343eb91bc6647a ``` aber es kann sein, dass der bei dir anders heißt. Sonst ist das einfach der zuletzt geänderte Ordner( Falls du schon einen Agenten trainiert hast) -> ```diff```-> ```adversarial-policies``` -> ```data```\n",
    "\n",
    "Hier liegen jetzt in ```baselines``` die trainierten models. In ```sacred``` werden die logs der einzelnen Trainingssessions in ``` train ``` sowie die logs der einzelen Scoresessions in ```score``` gespeichert. Wenn bei ```aprl.score videos=True``` gesetzt wurde speichert der die auch in die einzelnen score Order ab. Der Path kann aber angepasst werden.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
