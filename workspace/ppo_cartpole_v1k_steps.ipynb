{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\" [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/), \n",
    "\n",
    "<s>[Max episode length is 500](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), therefore the max reward is also 500.</s>\n",
    "\n",
    "This is a custom version of CartPole that has a maximum for episode steps of 1000.\n",
    "\n",
    "[Registering custom environments](https://stackoverflow.com/questions/42787924/why-is-episode-done-after-200-time-steps-gym-environment-mountaincar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to re-run unless output is cleared; registration is saved in PATH\\TO\\ENV\\lib\\site-packages\\gym\\envs\\registration.py\n",
    "# Re-running will throw UserWarning: PATH\\TO\\ENV\\lib\\site-packages\\gym\\envs\\registration.py:595: \n",
    "# UserWarning: WARN: Overriding environment CartPole-1k\n",
    "gym.envs.register(\n",
    "    id=\"CartPole-v1k\",\n",
    "    entry_point=\"gym.envs.classic_control:CartPoleEnv\",\n",
    "    max_episode_steps=1000, # CartPole-v1 uses 500\n",
    "    reward_threshold=975.0, # CartPole-v1 uses 475.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1k')\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward (agent trained over 1'000 timesteps): 330.41 +/- 270.76\n",
      "mean_reward (agent trained over 2'000 timesteps): 399.02 +/- 264.97\n",
      "mean_reward (agent trained over 3'000 timesteps): 349.32 +/- 229.78\n",
      "mean_reward (agent trained over 4'000 timesteps): 413.11 +/- 246.12\n",
      "mean_reward (agent trained over 5'000 timesteps): 421.51 +/- 228.71\n",
      "mean_reward (agent trained over 6'000 timesteps): 454.90 +/- 217.08\n",
      "mean_reward (agent trained over 7'000 timesteps): 475.73 +/- 256.68\n",
      "mean_reward (agent trained over 8'000 timesteps): 601.47 +/- 253.89\n",
      "mean_reward (agent trained over 9'000 timesteps): 764.44 +/- 250.46\n",
      "mean_reward (agent trained over 10'000 timesteps): 599.67 +/- 219.13\n",
      "mean_reward (agent trained over 11'000 timesteps): 823.81 +/- 212.27\n",
      "mean_reward (agent trained over 12'000 timesteps): 896.98 +/- 150.79\n",
      "mean_reward (agent trained over 13'000 timesteps): 902.53 +/- 154.34\n",
      "mean_reward (agent trained over 14'000 timesteps): 951.84 +/- 96.14\n",
      "mean_reward (agent trained over 15'000 timesteps): 942.05 +/- 108.92\n",
      "mean_reward (agent trained over 16'000 timesteps): 983.54 +/- 53.59\n",
      "mean_reward (agent trained over 17'000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 18'000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 19'000 timesteps): 1000.00 +/- 0.00\n",
      "mean_reward (agent trained over 20'000 timesteps): 1000.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,21):\n",
    "\n",
    "    # Train the agent for 1'000 steps\n",
    "    model.learn(total_timesteps=1000) \n",
    "\n",
    "    # Evaluate the trained agent - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "    mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "    print(f\"mean_reward (agent trained over {i}'000 timesteps): {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    # Save model\n",
    "    save_dir = \"./models/\"\n",
    "    model.save(save_dir + f\"/cartpole-v1k/cartpole_v1k_ppo_{i}K\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
