{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import CnnPolicy\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from utils.adversary_env import AdversaryEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Load target model\n",
    "target_model = A2C.load(\"../a2c/model/lunarlander_v2_a2c_3M_to_11M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target model - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "# mean_reward, std_reward = evaluate_policy(target_model, Monitor(gym.make('LunarLander-v2')), n_eval_episodes=50)\n",
    "# print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adversary with adversary env\n",
    "# adversary_env = AdversaryEnv(\"LunarLander-v2\", target_model)\n",
    "# adversary = PPO(MlpPolicy, adversary_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train adversary\n",
    "# log_dir = \"./out/training-log/\"\n",
    "# log_path = log_dir + f\"adversary-lunarlander-v2/\"\n",
    "# save_dir = \"./models/adversary-lunarlander-v2/\"\n",
    "\n",
    "# timesteps = 50000\n",
    "\n",
    "# adversary.learn(\n",
    "#         total_timesteps=timesteps,\n",
    "#         callback=None,\n",
    "#         log_interval=-1,\n",
    "#         eval_env=adversary_env, \n",
    "#         eval_freq=5000, \n",
    "#         n_eval_episodes=100,\n",
    "#         tb_log_name='PPO',\n",
    "#         eval_log_path=log_path, \n",
    "#         reset_num_timesteps=False\n",
    "#         )\n",
    "\n",
    "# # Save adversary\n",
    "# adversary.save(save_dir + f\"adversary_lunarlander_v2_{timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adversary\n",
    "save_dir = \"./models/adversary-lunarlander-v2/\"\n",
    "\n",
    "adversary = PPO.load(save_dir + f\"adversary_lunarlander_v2_50000\")\n",
    "adversary_env = AdversaryEnv('LunarLander-v2', target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.env.lander = b2Body(active=True,\n",
      "       angle=0.004038316663354635,\n",
      "       angularDamping=0.0,\n",
      "       angularVelocity=0.19957013428211212,\n",
      "       awake=True,\n",
      "       bullet=False,\n",
      "       contacts=[],\n",
      "       fixedRotation=False,\n",
      "       fixtures=[b2Fixture(body=b2Body(active=True,\n",
      "                                      angle=0.004038316663354635,\n",
      "                                      angularDamping=0.0,\n",
      "                                      angularVelocity=0.19957013428211212,...  )],\n",
      "       inertia=0.8333148956298828,\n",
      "       joints=[b2JointEdge(joint=b2RevoluteJoint(active=True,\n",
      "                                                anchorA=b2Vec2(9.96521,13.3213),\n",
      "                                                anchorB=b2Vec2(9.96521,13.3213),...  )],\n",
      "       linearDamping=0.0,\n",
      "       linearVelocity=b2Vec2(-1.76209,-0.909569),\n",
      "       localCenter=b2Vec2(0,0.101307),\n",
      "       mass=4.816666603088379,\n",
      "       massData=I=0.8333148956298828,center=b2Vec2(0,0.101307),mass=4.816666603088379,),\n",
      "       position=b2Vec2(9.96521,13.3213),\n",
      "       sleepingAllowed=True,\n",
      "       transform=R=<Box2D.Box2D.b2Rot; proxy of <Swig Object of type 'b2Rot *' at 0x000001B8B815E420> >,angle=0.004038316663354635,position=b2Vec2(9.96521,13.3213),),\n",
      "       type=2,\n",
      "       userData=None,\n",
      "       worldCenter=b2Vec2(9.9648,13.4226),\n",
      "       )\n",
      "env_copy1.lander = None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'angle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\data\\documents\\university\\projektgruppe-kivs\\brAIn\\workspace\\adv_attacks\\test_strategically_timed_attack.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_strategically_timed_attack.ipynb#ch0000005?line=25'>26</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mLunarLander-v2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_strategically_timed_attack.ipynb#ch0000005?line=26'>27</a>\u001b[0m ua \u001b[39m=\u001b[39m StrategicallyTimedAttack(env\u001b[39m=\u001b[39menv, model\u001b[39m=\u001b[39mtarget_model, attack\u001b[39m=\u001b[39madversary, epsilon\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m, beta\u001b[39m=\u001b[39mbeta)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_strategically_timed_attack.ipynb#ch0000005?line=27'>28</a>\u001b[0m ua\u001b[39m.\u001b[39;49mperform_attack()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_strategically_timed_attack.ipynb#ch0000005?line=29'>30</a>\u001b[0m all_episodes_rewards\u001b[39m.\u001b[39mappend(ua\u001b[39m.\u001b[39mreward_total)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_strategically_timed_attack.ipynb#ch0000005?line=30'>31</a>\u001b[0m all_episodes_perturbation\u001b[39m.\u001b[39mappend(ua\u001b[39m.\u001b[39mperturbation_total)\n",
      "File \u001b[1;32mg:\\data\\documents\\university\\projektgruppe-kivs\\brAIn\\workspace\\adv_attacks\\strategically_timed_attack.py:25\u001b[0m, in \u001b[0;36mStrategicallyTimedAttack.perform_attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=22'>23</a>\u001b[0m adv_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcraft_sample(orig_obs, orig_act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=23'>24</a>\u001b[0m perturbed_act, _states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(adv_sample)\n\u001b[1;32m---> <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc(orig_act, perturbed_act) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta:\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=25'>26</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperform_step(perturbed_act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=26'>27</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mg:\\data\\documents\\university\\projektgruppe-kivs\\brAIn\\workspace\\adv_attacks\\strategically_timed_attack.py:42\u001b[0m, in \u001b[0;36mStrategicallyTimedAttack.c\u001b[1;34m(self, orig_act, perturbed_act)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00menv_copy1\u001b[39m.\u001b[39mlander \u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=40'>41</a>\u001b[0m env_copy2 \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv)\n\u001b[1;32m---> <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=41'>42</a>\u001b[0m _state, max_act, _done, _info \u001b[39m=\u001b[39m env_copy1\u001b[39m.\u001b[39;49mstep(orig_act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=42'>43</a>\u001b[0m _state, min_act, _done, _info \u001b[39m=\u001b[39m env_copy2\u001b[39m.\u001b[39mstep(perturbed_act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/strategically_timed_attack.py?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m max_act \u001b[39m-\u001b[39m min_act\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/time_limit.py?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/time_limit.py?line=16'>17</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/time_limit.py?line=17'>18</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/time_limit.py?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/order_enforcing.py?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/order_enforcing.py?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/order_enforcing.py?line=12'>13</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/wrappers/order_enforcing.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:333\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=327'>328</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=328'>329</a>\u001b[0m         action\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=329'>330</a>\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=331'>332</a>\u001b[0m \u001b[39m# Engines\u001b[39;00m\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=332'>333</a>\u001b[0m tip \u001b[39m=\u001b[39m (math\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlander\u001b[39m.\u001b[39;49mangle), math\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle))\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=333'>334</a>\u001b[0m side \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mtip[\u001b[39m1\u001b[39m], tip[\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/gym/envs/box2d/lunar_lander.py?line=334'>335</a>\u001b[0m dispersion \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnp_random\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39m+\u001b[39m\u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m SCALE \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'angle'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from strategically_timed_attack import StrategicallyTimedAttack\n",
    "\n",
    "# Evaluate strategically timed adversarial attack over different beta\n",
    "data_dict = {\n",
    "    'beta': [],\n",
    "    'mean_rew': [],\n",
    "    'mean_perturbation': [],\n",
    "    'mean_n_timesteps': [],\n",
    "    'mean_n_attacks': []\n",
    "}\n",
    "\n",
    "n_episodes = 50\n",
    "\n",
    "for j in range(1, 50):\n",
    "    beta = j\n",
    "\n",
    "    all_episodes_rewards = []\n",
    "    all_episodes_perturbation = []\n",
    "    all_episodes_n_timesteps = []\n",
    "    all_episodes_n_attacks = []\n",
    "\n",
    "    for i in range(0, n_episodes):\n",
    "        # Run attack on a2c model\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        ua = StrategicallyTimedAttack(env=env, model=target_model, attack=adversary, epsilon=0.25, beta=beta)\n",
    "        ua.perform_attack()\n",
    "\n",
    "        all_episodes_rewards.append(ua.reward_total)\n",
    "        all_episodes_perturbation.append(ua.perturbation_total)\n",
    "        all_episodes_n_timesteps.append(ua.frames_count)\n",
    "        all_episodes_n_attacks.append(ua.n_attacks)\n",
    "\n",
    "        ua.reset_attack()\n",
    "\n",
    "    data_dict['beta'].append(beta)\n",
    "    data_dict['mean_rew'].append(np.round(np.mean(all_episodes_rewards), 2))\n",
    "    data_dict['mean_perturbation'].append(np.round(np.mean(all_episodes_perturbation), 2))\n",
    "    data_dict['mean_n_timesteps'].append(np.round(np.mean(all_episodes_n_timesteps), 2))\n",
    "    data_dict['mean_n_attacks'].append(np.round(np.mean(all_episodes_n_attacks), 2))\n",
    "\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\n",
    "        f\"Beta {data_dict['beta'][-1]}: \\n \\\n",
    "        {data_dict['mean_rew'][-1]} mean rew, \\n \\\n",
    "        {data_dict['mean_perturbation'][-1]} mean perturbation \\n \\\n",
    "        {data_dict['mean_n_timesteps'][-1]} mean n timesteps \\n \\\n",
    "        {data_dict['mean_n_attacks'][-1]} mean n attacks\"\n",
    "        )\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "labels = ['beta', 'mean_rew', 'mean_perturbation', 'mean_n_timsteps', 'mean_n_attacks']\n",
    "\n",
    "data_df = pd.DataFrame(\n",
    "        data_dict, \n",
    "        columns=labels,\n",
    "        )\n",
    "data_df.set_index('beta', inplace=True)\n",
    "\n",
    "# Save data as .csv file\n",
    "with open(\"./out/data/\" + \"strategically_timed_attack_beta\", 'w') as f:\n",
    "    data_df.to_csv(f)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load data\n",
    "with open(\"./out/data/\" + \"strategically_timed_attack_beta\", 'r') as f:\n",
    "    data_df = pd.read_csv(f, index_col=0)\n",
    "\n",
    "# Plot data_df\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(data_df.index, data_df['mean_rew'], color='darkslategray', alpha=1, zorder=0)\n",
    "plt.vlines(data_df.index, ymin=0, ymax=data_df['mean_rew'], color=np.where(data_df['mean_rew'] < 0, 'maroon', 'darkgreen'), alpha=1, zorder=1)\n",
    "plt.scatter(data_df.index, data_df['mean_rew'], color=np.where(data_df['mean_rew'] < 0, 'maroon', 'darkgreen'), label='mean reward', marker='D', alpha=1, zorder=2)\n",
    "\n",
    "plt.plot(data_df.index, data_df['mean_perturbation'], label='mean perturbation', color='crimson', linestyle='dotted', alpha=1, zorder=0)\n",
    "\n",
    "# plt.plot(data_df.index, data_df['mean_n_timesteps'], label='mean n timesteps', color='purple', linestyle='dotted', alpha=1, zorder=0)\n",
    "\n",
    "plt.plot(data_df.index, data_df['mean_n_attacks'], label='mean n attacks', color='black', linestyle='dashed', alpha=1, zorder=0)\n",
    "\n",
    "plt.title(\"Strategically timed attack\")\n",
    "plt.legend()\n",
    "plt.xlabel('$\\\\beta$')\n",
    "# plt.ylabel()\n",
    "plt.axis([0.01, 0.49, -300, 500]) # plt.axis([xmin, xmax, ymin, ymax])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
