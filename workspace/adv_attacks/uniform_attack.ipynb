{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "save_dir = \"./models/ppo\"\n",
    "model = PPO.load(save_dir + f\"/cartpole-v1/cartpole_v1_ppo_100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward : 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform attack on CartPole-v1\n",
    "def perturbate(env, obs, perturbation_weight):\n",
    "    \"\"\"Perturbate the observation to hinder the agent.\"\"\"\n",
    "\n",
    "    # Threshholds\n",
    "    # 2.4 for x\n",
    "    # 12 * 2 * math.pi / 360 for theta\n",
    "    x_threshold = env.x_threshold\n",
    "    theta_threshold = env.theta_threshold_radians\n",
    "    \n",
    "    # Get the cart's x position and pole's angle \n",
    "    x = obs[0]\n",
    "    theta = obs[2]\n",
    "\n",
    "    # Calculate which variable is closer to termination condition\n",
    "    x_danger = np.absolute((x_threshold - np.absolute(x)) / x_threshold)\n",
    "    theta_danger = np.absolute((theta_threshold / 360 - np.absolute(theta)) / theta_threshold)\n",
    "    \n",
    "    # print(f\"x: {x}, danger: {x_danger}\")\n",
    "    # print(f\"theta: {theta}, danger: {theta_danger}\")\n",
    "\n",
    "    # Actions are encoded as \n",
    "    # '0' -> push cart to the left\n",
    "    # '1' -> push card to the right\n",
    "\n",
    "    # Calculate the target action to minimize reward\n",
    "    # The following favors theta over x in case of equal danger\n",
    "    if x_danger > theta_danger:\n",
    "        if x < 0:\n",
    "            # If x is already to the left -> target action: move left\n",
    "            # Therefore pretend x is more to the right and pole is angled more to the left\n",
    "            x_perturbation = 1 * perturbation_weight[0]\n",
    "            theta_perturbation = -1 * perturbation_weight[1]\n",
    "        else:\n",
    "            x_perturbation = -1 * perturbation_weight[0]\n",
    "            theta_perturbation = 1 * perturbation_weight[1]\n",
    "            \n",
    "    else:\n",
    "        if theta < 0:\n",
    "            # Pole is angled to the left -> target action: move right\n",
    "            # Therefore pretend x is more to the left and pole is angled more to the right\n",
    "            x_perturbation = -1 * perturbation_weight[0]\n",
    "            theta_perturbation = 1 * perturbation_weight[1]\n",
    "        else:\n",
    "            x_perturbation = 1 * perturbation_weight[0]\n",
    "            theta_perturbation = -1 * perturbation_weight[1]\n",
    "\n",
    "    # Generate adversarial sample to trick the agent to select target action\n",
    "    x = obs[0] + x_perturbation\n",
    "    x_dot = obs[1]\n",
    "    theta = obs[2] + theta_perturbation\n",
    "    theta_dot = obs[3]\n",
    "    \n",
    "    # CartPole-v1's state: (x, x_dot, theta, theta_dot)\n",
    "    state = (x, x_dot, theta, theta_dot)\n",
    "    \n",
    "    # Construct return value according to CartPole-v1 syntax\n",
    "    adversarial_sample = np.array(state, dtype=np.float32)\n",
    "\n",
    "    total_perturbation = np.absolute(obs - adversarial_sample)\n",
    "\n",
    "    return adversarial_sample, total_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Final mean reward: 9.42\n",
      "Mean perturbation: 1.79\n",
      "Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "all_episodes_rewards = []\n",
    "all_episodes_perturbation = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    episode_rewards = []\n",
    "    episode_perturbations = []\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        ### uniform attack ###\n",
    "        adversarial_sample, perturbation = perturbate(env, obs, [.01, 0.75])\n",
    "        ### uniform attack ###        \n",
    "        \n",
    "        action, _states = model.predict(adversarial_sample)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_perturbations.append(perturbation)\n",
    "\n",
    "    all_episodes_rewards.append(sum(episode_rewards))\n",
    "    all_episodes_perturbation.append(sum(episode_perturbations))\n",
    "    # print(f\"Mean reward over evaluation iteration {i}: {np.mean(sum(episode_rewards))}\")\n",
    "    # print(f\"Updated mean reward over all iterations: {np.mean(all_episodes_rewards)}\")\n",
    "    # print()\n",
    "\n",
    "print(f\"Finished!\")\n",
    "print(f\"Final mean reward: {np.mean(all_episodes_rewards):.2f}\")\n",
    "print(f\"Mean perturbation: {np.mean(all_episodes_perturbation):.2f}\")\n",
    "print(f\"Num episodes: {num_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
