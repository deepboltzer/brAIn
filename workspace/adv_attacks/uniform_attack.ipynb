{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "save_dir = \"./models/ppo\"\n",
    "model = PPO.load(save_dir + f\"/cartpole-v1/cartpole_v1_ppo_100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward : 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform attack on CartPole-v1\n",
    "def pertubate(env, obs):\n",
    "    \"\"\"Pertubate the observation to hinder the agent.\"\"\"\n",
    "\n",
    "    # Threshholds\n",
    "    # 2.4 for x\n",
    "    # 12 * 2 * math.pi / 360 for theta\n",
    "    x_threshold = env.x_threshold\n",
    "    theta_threshold = env.theta_threshold_radians\n",
    "    \n",
    "    # Get the cart's x position and pole's angle \n",
    "    x = obs[0]\n",
    "    theta = obs[2]\n",
    "\n",
    "    # Calculate which variable is closer to termination condition\n",
    "    x_danger = np.absolute((x_threshold - np.absolute(x)) / x_threshold)\n",
    "    theta_danger = np.absolute((theta_threshold / 360 - np.absolute(theta)) / theta_threshold)\n",
    "    \n",
    "    # print(f\"x: {x}, danger: {x_danger}\")\n",
    "    # print(f\"theta: {theta}, danger: {theta_danger}\")\n",
    "\n",
    "    # Actions are encoded as \n",
    "    # '0' -> push cart to the left\n",
    "    # '1' -> push card to the right\n",
    "\n",
    "    # Calculate the target action to minimize reward\n",
    "    # The following favors theta over x in case of equal danger\n",
    "    if x_danger > theta_danger:\n",
    "        if x < 0:\n",
    "            target_action =  0 # If x is already to the left go further to the left\n",
    "        elif x > 0:\n",
    "            target_action =  1 # If x is already to the right go further to the right\n",
    "        else: \n",
    "            target_action =  np.random.choice([0, 1]) # Note that this case suggests that x = theta = 0\n",
    "    else:\n",
    "        if theta < 0:\n",
    "            target_action =  1 # If the pole is angled to the left -> go to the right\n",
    "        elif theta > 0:\n",
    "            target_action =  0 # If the pole is angled to the right -> go to the left\n",
    "        else:\n",
    "            target_action =  np.random.choice([0, 1]) # Note that this case suggests that x = theta = 0\n",
    "\n",
    "    # Generate adversarial sample to trick the agent to select target action\n",
    "    x = -obs[0]\n",
    "    x_dot = -obs[1]\n",
    "    theta = -obs[2]\n",
    "    theta_dot = -obs[3]\n",
    "    \n",
    "    # CartPole-v1's state: (x, x_dot, theta, theta_dot)\n",
    "    state = (x, x_dot, theta, theta_dot)\n",
    "    \n",
    "    # Construct return value according to CartPole-v1 syntax\n",
    "    obs = np.array(state, dtype=np.float32)\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Final mean reward: 8.84\n",
      "Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "all_episodes_rewards = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    episode_rewards = []\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        ### uniform attack ###\n",
    "        pertubated_obs = pertubate(env, obs)\n",
    "        ### uniform attack ###        \n",
    "        \n",
    "        action, _states = model.predict(pertubated_obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "    all_episodes_rewards.append(sum(episode_rewards))\n",
    "    # print(f\"Mean reward over evaluation iteration {i}: {np.mean(sum(episode_rewards))}\")\n",
    "    # print(f\"Updated mean reward over all iterations: {np.mean(all_episodes_rewards)}\")\n",
    "    # print()\n",
    "\n",
    "print(f\"Finished!\\nFinal mean reward: {np.mean(all_episodes_rewards)}\\nNum episodes: {num_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
