{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import uniform_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "current_dir = os.getcwd()\n",
    "save_dir = os.path.abspath(os.path.join(current_dir, os.pardir)) + \"/ppo/models/\"\n",
    "model = PPO.load(save_dir + \"/cartpole-v1/cartpole_v1_ppo_15000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward : 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model - for info on why the env is wrapped with Monitor check the evaluate_policy function\n",
    "mean_reward, std_reward = evaluate_policy(model, Monitor(env), n_eval_episodes=100)\n",
    "print(f\"mean_reward : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Final mean reward: 10.31\n",
      "Mean perturbation: 1.96\n",
      "Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "all_episodes_rewards = []\n",
    "all_episodes_perturbation = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    episode_rewards = []\n",
    "    episode_perturbations = []\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        ### uniform attack ###\n",
    "        adversarial_sample, perturbation = uniform_attack.perturbate(env, obs, [.01, 0.75])\n",
    "        ### uniform attack ###        \n",
    "        \n",
    "        action, _states = model.predict(adversarial_sample)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_perturbations.append(perturbation)\n",
    "\n",
    "    all_episodes_rewards.append(sum(episode_rewards))\n",
    "    all_episodes_perturbation.append(sum(episode_perturbations))\n",
    "\n",
    "print(f\"Finished!\")\n",
    "print(f\"Final mean reward: {np.mean(all_episodes_rewards):.2f}\")\n",
    "print(f\"Mean perturbation: {np.mean(all_episodes_perturbation):.2f}\")\n",
    "print(f\"Num episodes: {num_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
