{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import CnnPolicy\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from utils.adversary_env import AdversaryEnv\n",
    "from utils.adversary_wrapper import AdversaryWrapper\n",
    "\n",
    "# Load target model\n",
    "target_model = A2C.load(\"../a2c/model/lunarlander_v2_a2c_3M_to_11M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create adversary with adversary wrapper\n",
    "adversary_env = AdversaryWrapper(env=gym.make('LunarLander-v2'))\n",
    "adversary_env.set_model(target_model)\n",
    "adversary = PPO(MlpPolicy, adversary_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adversary with adversary env\n",
    "# adversary_env = AdversaryEnv(\"LunarLander-v2\", target_model)\n",
    "# adversary = PPO(MlpPolicy, adversary_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape () for Box environment, please use (8,) or (n_env, 8) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\data\\documents\\university\\projektgruppe-kivs\\brAIn\\workspace\\adv_attacks\\test_attack.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=3'>4</a>\u001b[0m save_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./models/adversary-lunarlander-v2/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=5'>6</a>\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m15000\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=7'>8</a>\u001b[0m adversary\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=8'>9</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtimesteps\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=9'>10</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39m# Save adversary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/data/documents/university/projektgruppe-kivs/brAIn/workspace/adv_attacks/test_attack.ipynb#ch0000003?line=12'>13</a>\u001b[0m adversary\u001b[39m.\u001b[39msave(save_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madversary_lunarlander_v2_\u001b[39m\u001b[39m{\u001b[39;00mtimesteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=285'>286</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=286'>287</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=287'>288</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=295'>296</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=296'>297</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=299'>300</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=300'>301</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=301'>302</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=302'>303</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=303'>304</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=304'>305</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=305'>306</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=306'>307</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=307'>308</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/ppo/ppo.py?line=308'>309</a>\u001b[0m     )\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=245'>246</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=247'>248</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=249'>250</a>\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=251'>252</a>\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=252'>253</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=174'>175</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=175'>176</a>\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=177'>178</a>\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=179'>180</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=181'>182</a>\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=154'>155</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=155'>156</a>\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=156'>157</a>\u001b[0m \n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=157'>158</a>\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=158'>159</a>\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=159'>160</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=160'>161</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=161'>162</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=42'>43</a>\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=43'>44</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=44'>45</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=46'>47</a>\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=47'>48</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/monitor.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/monitor.py?line=88'>89</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/monitor.py?line=89'>90</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/monitor.py?line=90'>91</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/monitor.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mg:\\data\\documents\\university\\projektgruppe-kivs\\brain\\workspace\\utils\\adversary_wrapper.py:22\u001b[0m, in \u001b[0;36mAdversaryWrapper.step\u001b[1;34m(self, act)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brain/workspace/utils/adversary_wrapper.py?line=19'>20</a>\u001b[0m \u001b[39m\"\"\"Step in environment based on models prediction of adversary's action.\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brain/workspace/utils/adversary_wrapper.py?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(act)\n\u001b[1;32m---> <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brain/workspace/utils/adversary_wrapper.py?line=21'>22</a>\u001b[0m model_act, _states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brain/workspace/utils/adversary_wrapper.py?line=22'>23</a>\u001b[0m next_state, rew, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(model_act)\n\u001b[0;32m     <a href='file:///g%3A/data/documents/university/projektgruppe-kivs/brain/workspace/utils/adversary_wrapper.py?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_action \u001b[39m=\u001b[39m act\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:562\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=541'>542</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=542'>543</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=543'>544</a>\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=546'>547</a>\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=547'>548</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=548'>549</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=549'>550</a>\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=550'>551</a>\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=559'>560</a>\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=560'>561</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/base_class.py?line=561'>562</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\policies.py:335\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=326'>327</a>\u001b[0m \u001b[39m# TODO (GH/1): add support for RNN policies\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=327'>328</a>\u001b[0m \u001b[39m# if state is None:\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=328'>329</a>\u001b[0m \u001b[39m#     state = self.initial_state\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=329'>330</a>\u001b[0m \u001b[39m# if episode_start is None:\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=330'>331</a>\u001b[0m \u001b[39m#     episode_start = [False for _ in range(self.n_envs)]\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=331'>332</a>\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=332'>333</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=334'>335</a>\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=337'>338</a>\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\policies.py:250\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=245'>246</a>\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(observation)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=247'>248</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=248'>249</a>\u001b[0m     \u001b[39m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=249'>250</a>\u001b[0m     vectorized_env \u001b[39m=\u001b[39m is_vectorized_observation(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=250'>251</a>\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/policies.py?line=251'>252</a>\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\utils.py:375\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[1;34m(observation, observation_space)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=372'>373</a>\u001b[0m \u001b[39mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[39min\u001b[39;00m is_vec_obs_func_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=373'>374</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, space_type):\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=374'>375</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m is_vec_obs_func(observation, observation_space)\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=375'>376</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=376'>377</a>\u001b[0m     \u001b[39m# for-else happens if no break is called\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=377'>378</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mH:\\Programs\\Anaconda\\envs\\torch-gym\\lib\\site-packages\\stable_baselines3\\common\\utils.py:242\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[1;34m(observation, observation_space)\u001b[0m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=239'>240</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=240'>241</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=241'>242</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=242'>243</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Unexpected observation shape \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=243'>244</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBox environment, please use \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=244'>245</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mor (n_env, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) for the observation shape.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, observation_space\u001b[39m.\u001b[39mshape)))\n\u001b[0;32m    <a href='file:///h%3A/Programs/Anaconda/envs/torch-gym/lib/site-packages/stable_baselines3/common/utils.py?line=245'>246</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Error: Unexpected observation shape () for Box environment, please use (8,) or (n_env, 8) for the observation shape."
     ]
    }
   ],
   "source": [
    "# Train adversary\n",
    "log_dir = \"./out/training-log/\"\n",
    "log_path = log_dir + f\"adversary-lunarlander-v2/\"\n",
    "save_dir = \"./models/adversary-lunarlander-v2/\"\n",
    "\n",
    "timesteps = 15000\n",
    "\n",
    "adversary.learn(\n",
    "        total_timesteps=timesteps\n",
    "        )\n",
    "\n",
    "# Save adversary\n",
    "adversary.save(save_dir + f\"adversary_lunarlander_v2_{timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load adversary\n",
    "# log_dir = \"./out/training-log/\"\n",
    "# log_path = log_dir + f\"adversary-lunarlander-v2/\"\n",
    "# save_dir = \"./models/adversary-lunarlander-v2/\"\n",
    "\n",
    "# adversary = PPO.load(save_dir + f\"adversary_lunarlander_v2_15000\")\n",
    "# adversary_env = AdversaryEnv(\"LunarLander-v2\", target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from uniform_attack import UniformAttack\n",
    "\n",
    "# # Run attack on a2c model\n",
    "# env = gym.make(\"LunarLander-v2\")\n",
    "# ua = UniformAttack(env=env, model=target_model, attack=adversary)\n",
    "# ua.perform_attack()\n",
    "# print(f\"Total reward achieved: {ua.reward_total}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1eaab1189d37ae29f7eac77705a074d5028424b72ba552afc8bf8536055072c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
