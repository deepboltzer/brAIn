{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First post\n",
    "Exciting breakthroughs have been made in Artifical Intelligence using Reinforcement Learning in the past years. Naturally there is value in looking into how it exactly works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is Reinforcement Learning?\n",
    "To understand the fundamentals of Reinforcement Learning (RL) it's important to understand one key concept: The relationship between an agent and its environment.\n",
    "To grasp the idea of an agent, it seems pretty natural to replace it with oneself - a human being interacting with its surroundings, retrieving information about itself and those surroundings, and changing this state through actions. \n",
    "\n",
    "Now we - as the agent - usually have quite a lot of different actions we could take. Imagine having to walk with the dog: There are two options regarding clothing. We could either take a jacket or pass on doing so. To decide which action to pick, it's pretty instinctively to observe the temperature outside - the environment so to speak. But how do we know if our decision was good in that scenario? We'll receive a feedback - a reward - from the environment. In our example we might be freezing if we decided not to take the jacket.\n",
    "\n",
    "And that example - at its core - is exactly what the relationship between agent and environment is about. More formally an agent takes an action *a* in state *s* and collects a reward *r* in return to evaluate how good of an action *a* was in that specific state *s* and additionally transitioning into the next state *s'*. A transitionfunction *P* decides which state is going to be the next state *s'*.\n",
    "\n",
    "// MDP?\n",
    "\n",
    "// insert agent - environment illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based vs model-free\n",
    "Now that we introduced the key concept of an agent - environment relationship it's necessary to differentiate between to general cases: Knowing the model and not knowing the model.\n",
    "\n",
    "Imagine a world, in which the weather is sunny exactly every other day and rainy on every remaining day. If our agent has to do a task outside but can freely decide on which day to do it, knowing the model of its world it will probably decide to go outside on a sunny day for a better outcome.\n",
    "If it - on the other hand - does not know the model it will probably go out on the first day to collect the reward for completing its task as soon as possible and will only over the course of weeks learn the model of its world as part of the learning experience itself.\n",
    "\n",
    "Again more formally there is a distinction between model-free and model-based RL. In model-based RL the model is known, the agent has all the information and can plan its actions perfectly. That's why model-based problems can be solved with \"simpler\" algorithms using Dynamic Programming.\n",
    "Model-free RL on the other hand needs the agent to learn the model itself as part of solving the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions\n",
    "Now to evaluate an action *a* in a specific state *s* it doesn't seem too practicable to try it out and see what the reward will be. Instead it seems to be a good idea to try and predict what the future rewards will be if action *a* is taken in state *s*.\n",
    "\n",
    "To do that let's introduce the value function $G_t$. Essentially it sums up all futuring rewards starting with time *t+1* - so right after action *a* was taken.\n",
    "Going back to our own decision making, it's pretty intuitive to prefer a reward right now rather than further down the line, e.g. because the latter might be more uncertain. To include this in the value function we discount future rewards using a discount factor $\\gamma$. That leaves us with \n",
    "\n",
    "$G_t = \\sum_{k=0} \\gamma^k R_{t+k+1}$.\n",
    "\n",
    "Now we can either extend this function by adding in the current state *s* giving us the state-value function:\n",
    "\n",
    "$V(s) = \\textbf{E}\\left[ G_t | S_t = s \\right]$\n",
    "\n",
    "or both state *s* and action *a* in a state-action pair giving us the action-value:\n",
    "\n",
    "$Q(s,a) = \\textbf{E}\\left[ G_t | S_t = s, A_t = a \\right]$\n",
    "\n",
    "// policy $\\pi$ erst einmal bewusst weggelassen, da für Bellman Gleichung nicht unbedingt notwendig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "The Bellman equation is used to decompose the value function into the immediate reward and the discounted futuring values.\n",
    "\n",
    "This can be done for the state-value function:\n",
    "\n",
    "$V(s) = \\textbf{E}\\left[ R_{t+1} + \\gamma V(S_{t+1}) | S_t = s \\right]$\n",
    "\n",
    "and the action-value function respectively:\n",
    "\n",
    "$Q(s, a) = \\textbf{E}\\left[ R_{t+1} + \\gamma Q(S_{t+1}) | S_t = s \\right]$\n",
    "\n",
    "// Hier würden jetzt spätestens policies benötigt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab61a2e4d8b782f4fa1392fa26dc29674c38a23c0d12aa486ac5eb217fa6cc10"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
