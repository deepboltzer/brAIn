{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: A \"mathless\" introduction to underlying concepts\n",
    "This first blog post is intended to give you a rough outline of key concepts of Reinforcment Learning. To achieve that we will try to introduce them naturally by addressing your intuition. While this post will attempt to be perfectly understandable without any special mathematical knowledge, math, definitions and equations are necessary and unremovable from Reinforcement Learning and will be introduced in a second post.\n",
    "\n",
    "// Maybe change \"you\" to \"the reader\", also \"we\" for something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A motivation: Why bother with Reinforcement Learning?\n",
    "Looking back at recent historic successes of reinforcement learning algorithms there are various use cases for such techniques that become quite obvious. The most well known is probably AlphaGo, a computer program developed by researchers at DeepMind, which has been shown to surpass the abilities of th best professional human Go player in 2017. Additional success stories are to be found in the realm of esports. OpenAI developed bots able to beat professional teams at the highly competetive video game Dota 2 in a 5v5 game in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is Reinforcement Learning?\n",
    "To understand the fundamentals of Reinforcement Learning it's important to understand one key concept: The relationship between an agent and its environment. To grasp the idea of an agent, it seems pretty natural to replace it with oneself - a human being interacting with its surroundings, retrieving information about itself and those surroundings, and changing this state through actions.\n",
    "\n",
    "Now we - as the agent - usually have quite a lot of different actions we could take. Imagine having to walk with the dog: There are two options regarding clothing. We could either take a jacket or pass on doing so. To decide which action to pick, it's pretty instinctively to observe the temperature outside - the environment so to speak. But how do we know if our decision was good in that scenario? We'll receive a feedback - a reward - from the environment. In our example we might be freezing if we decided not to take the jacket.\n",
    "\n",
    "And that example - at its core - is exactly what the relationship between agent and environment is about. More formally an agent takes an action ***a*** in state ***s*** and collects a reward ***r*** in return to evaluate how good of an action a was in that specific state ***s*** and additionally transitioning into the next state **s'** which is determined by a transition function ***P***.\n",
    "\n",
    "// Insert first illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based vs model-free\n",
    "Now that we introduced the key concept of an agent - environment relationship it's necessary to differentiate between to general cases: Knowing the model and not knowing the model.\n",
    "\n",
    "Imagine a world, in which the weather is sunny exactly every other day and rainy on every remaining day. If our agent has to do a task outside but can freely decide on which day to do it, knowing the model of its world it will probably decide to go outside on a sunny day for a better outcome.\n",
    "If it - on the other hand - does not know the model it will probably go out on the first day to collect the reward for completing its task as soon as possible and will only over the course of weeks learn the model of its world as part of the learning experience itself.\n",
    "\n",
    "Again more formally there is a distinction between model-free and model-based Reinforcement Learning. In model-based Reinforcement Learning the model is known, the agent has all the information and can plan its actions perfectly. That's why model-based problems can be solved with \"simpler\" algorithms using Dynamic Programming.\n",
    "Model-free Reinforcement Learning on the other hand needs the agent to learn the model itself as part of solving the problem.\n",
    "\n",
    "// Maybe Insert another illustration, expanded with the model but without policies yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "// Introduce $\\pi$\n",
    "\n",
    "A policy $\\pi$ first of all describes the strategy by which the agent will try to maximize its rewards. Generally there are two cases: on-policy learning and off-policy learning.\n",
    "With on-policy learning the agent chooses an action based on the target policy while off-policy works based off a different so called behavior policy rather than the actual target policy.\n",
    "\n",
    "Furthermore we differentiate a deterministic policy $\\pi$(***s***) and a stochastic policy $\\pi$(***a***|***s***) which returns the action based on a probability given that the agent is in state ***s***.\n",
    "\n",
    "// ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions\n",
    "\n",
    "// Probably without introducing the *actual* functions and rather speaking about the idea. Also introduce the discounting factor $\\gamma$\n",
    "\n",
    "Now to evaluate an action ***a*** in a specific state ***s*** it doesn't seem too practicable to try it out and see what the reward will be. Instead it seems to be a good idea to try and predict what the future rewards will be if action ***a*** is taken in state ***s***.\n",
    "\n",
    "To do that let's introduce the value function $G_t$. Essentially it sums up all futuring rewards starting with time ***t+1*** - so right after action ***a*** was taken.\n",
    "Going back to our own decision making, it's pretty intuitive to prefer a reward right now rather than further down the line, e.g. because the latter might be more uncertain. To include this in the value function we discount future rewards using a discount factor $\\gamma$.\n",
    "\n",
    "// Then use that to introduce the idea of updating the policy\n",
    "\n",
    "// Insert illustration as shown in Luca's sketch: Environment - Agent - value function - Policy for model-based and model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//TODO: Introduce episodes?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab61a2e4d8b782f4fa1392fa26dc29674c38a23c0d12aa486ac5eb217fa6cc10"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
