{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a772bc",
   "metadata": {},
   "source": [
    "# Collection of Formulas\n",
    "\n",
    "### This file contains all the mathematical equations that has been used in the posts.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb58e29",
   "metadata": {},
   "source": [
    "__Timestep__: $$t= 1,2,3,...,T$$<br> denotes the __time__ and is used to label a sequence of actions and rewards. <br>\n",
    "\n",
    "__Episode__: $$\\tau := s_1,a_1,r_2,s_2,a_2,r_3...s_{T-1},a_{T-1},r_T,s_T$$<br> a fully observed sequence that ends in terminal state __$s_T$__. <br>\n",
    "\n",
    "__Return__: $$G_t := r_{t+1}+\\gamma r_{t+2}+ \\gamma^2 r_{t+3}+... = \\sum_{k=0}^{\\infty} \\gamma^kr_{t+k+1}$$<br> it is used to  measure the importance of the immediate reward and future rewards with $\\gamma$ being the discounting factor.<br>\n",
    "\n",
    "\n",
    "__Transition function__: $$P(s',r\\vert s,a)$$<br> it is used to calculate the probabilty of the transition from $s$ to $s'$.<br>\n",
    "\n",
    "__State-Transition function__:$$P(s'\\vert s,a) := P^a_{s,s'} :=\\sum_{r\\in \\mathcal{R}}P(s',r\\vert s,a)$$<br> derived from transition function.<br>\n",
    "\n",
    "__Reward function__:$$ R(r\\vert s,a):= \\mathbb{E}[r, \\vert s,a] := \\sum_{r \\in \\mathcal{R}} r\\sum_{s' \\in \\mathcal{S}}P(s',r\\vert s,a)$$<br>derived from transition function and used to predict the next reward.<br>\n",
    "\n",
    "\n",
    "__Value function__:$$V(s)= \\mathbb{E}[G_t|S_t=s]$$ $$V_\\pi(s) := \\mathbb{E}_\\pi[G_t|S_t=s]$$ $$V_{*}(s) = \\max_{\\pi} V_{\\pi}(s),Q_{*}(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$$<br> used to compute the expected return with the condition of being in state $s$. $V_\\pi(s)$ is the Value function following policy $\\pi$ and $V_{*}(s)$ is the optimal Value function.<br>\n",
    "\n",
    "__Action value(Q-Value)__:$$Q(s, a) = \\mathbb{E}[G_t \\vert S_t = s, A_t = a]$$ $$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a]$$<br> a state-action pair function. The expected return is not only conditioned for the current state but also for the action. $Q_\\pi(s,a)$ defines the Q-Value following the policy $\\pi$. <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Bellman equations__:$$ V(s) = \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s]$$ $$V_\\pi(s)=\\mathbb{E}_\\pi[R_{t+1}+\\gamma V_\\pi(S_{t+1}) \\vert S_t = s] $$ $$V_*(s) = {max}_{a \\in A}Q_*(s,a)$$ <br>used to split up Value function $ V(s)$ into immediate reward plus the discounted value of future states. $V_\\pi(s)$ is the Bellman Expectation Equation following target policy $\\pi$. $V_*(s)$ is the Bellman Optimality Equation and it calculates the maximum expected return of the Value function.   <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
