{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88f554c",
   "metadata": {},
   "source": [
    "# Tactical Approaches to Adversarial attacks on RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa0631",
   "metadata": {},
   "source": [
    "## Adversarial attacks\n",
    "\n",
    "<!-- \n",
    "- fundamentals of adversarial attacks\n",
    "- keywords\n",
    "- goals of attacks\n",
    "- how agent and environment are affected \n",
    "- focus of this post \n",
    "-->\n",
    "\n",
    "First of all let us introduce the fundamental concepts of adversarial attacks. The overall goal of an adversarial attack is reducing the agent's reward to a minimum by manipulating its choices.\n",
    "\n",
    "In order to achieve this goal an adversarial attack impairs the performance of a trained model - in our case a RL trained model - by feeding it with false information. This so called ***adversarial sample*** usually consists of a perturbed version of the original observation which itself is returned by the environment. The adversarial sample manipulates the agent to take preferably the least desired action while also being similar enough to a valid observation to not be easily detectable. \n",
    "\n",
    "While the ***adversarial perturbation*** is the amount of noise added to the observation during the sample crafting, the instance or agent crafting the samples themselves is called ***adversary***. Furthermore we differentiate so called ***white-box attacks*** from ***black-box attacks***. Adversaries of the latter attack models of which they have no information. In some cases (cases in which the adversary has limited information about the target model but never its parameters) black-box attacks are further sub-classified in ***semi-black-box attacks***. \n",
    "\n",
    "This specific post will limit itself on ***tactical approaches*** to adversarial attacks as presented in Lin, *et al.* (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5ec45",
   "metadata": {},
   "source": [
    "## Different types of adversarial attacks\n",
    "\n",
    "<!-- \n",
    "( strategically timed to critical point)(maybe enchanting,antagonist)\n",
    "\n",
    "- explain basic idea(strategically timed and enchanting)\n",
    "- explain attack strategy and present functions(informal or formal)\n",
    "- effects on agent and environment\n",
    "\n",
    "- introduce critical point strategy( and antagonist attack) ( what are the differences?)\n",
    "- basic idea and principle\n",
    "- attack strategy\n",
    "- effect on agent and environment compared to strategically timed attack \n",
    "-->\n",
    "\n",
    "Starting off the most approachable and simple way to go about attacking an agent using adversarial methods is the ***uniform attack***. Here adversarial samples are crafted at each and every timestep. Therefore the agent is attacked a lot resulting in a large adversarial perturbation which somewhat defeats the idea of adversarial attacks being rather difficult to detect.\n",
    "\n",
    "### Strategically timed attack\n",
    "\n",
    "Lin, **et al.** introduce the idea of a so called ***strategically timed attack***. Even for simple examples it is quite intuitive that attacks are not equally efficient at different timesteps, meaning e.g. attacking an agent that acts in OpenAI Gym's **CarRacing** environment (introduced in more detail later on) would be less efficient during longer straight sections of the track compared to curved sections.\n",
    "To determine when the adversary is to craft an adversarial sample we first compute a function $c$ that essentially compares the rewards of the agent's best and worst action as follows:\n",
    "$$c(s_t) = \\max_{a_t}\\pi(s_t, a_t) - \\min_{a_t}\\pi(s_t, a_t)$$\n",
    "\n",
    "Note that this method of computing $c$ is only applicable for policy gradient-based methods like A3C or PPO.\n",
    "\n",
    "Next an adversarial sample is only crafted if $c$ at least matches a certain threshold $\\beta$. Overall the number of attacks during an episode depends on wether or not an adversarial sample was crafted in the individual timesteps and therefore directly on said threshold $\\beta$. Put simple a large threshold results in few attacks while a small threshold results in many attacks. This of course not only affects the overall adversarial perturbation but also the effectiveness of the adversarial attacks. Choosing $\\beta$ wisely therefore determines both the success of an adversary attack and its perceptibility. \n",
    "\n",
    "### Enchanting attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665951a",
   "metadata": {},
   "source": [
    "## Implementation and results\n",
    "- show process and results through implementation on example\n",
    "- compare results of attacks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76514a4c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- evaluate results of attacks\n",
    "- explain why critical point attack is more \"evolved\"\n",
    "- outlook on application of these attacks on Real world scenarios\n",
    "- (maybe comparison with other attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02411c",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Carlini and Wagner (2016)](https://ieeexplore.ieee.org/abstract/document/8294186)\n",
    "- [Lin, _et al._ (2017)](https://arxiv.org/abs/1703.06748)\n",
    "- [CarRacing](https://gym.openai.com/envs/CarRacing-v0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
