{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RL Concepts (TD and On- vs Off-policy learning)\n",
    "\n",
    "Today we will be looking at two RL algorithms which are very similar on the surface but show one key difference. Both algoithms learn the action-value function $Q(s, a)$ in order to find the best policy possible. Once $Q(s, a)$ has been learned and optimized, the policy $\\pi(a | s)$ can be derived by greedily chosing the best action $a$ leading to the highest value of $Q(s, a)$ when in state $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference Learning\n",
    "\n",
    "Both algorithms we look at today utilize **Temporal-Difference (TD)** Learning. TD-Learning that either the action-value function $Q(s, a)$ or the value funtion $V(s)$ are improved by shifting the existing function acording to the observed experiences. In this, TD-Learning shares some similiarties with stochastic gradient descent (SGD) as employed in neural networks, seeing as in each iteration SGD slightly shifts the parameters to gradually improve the neural network performance.\n",
    "\n",
    "Each TD-Learning step considers a sequence $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$ from a trajectory $\\tau$ at a timestep $t$. Further a hyperparameter $\\alpha \\in (0, 1)$ is needed and the discount of future rewards $\\gamma$ has to be taken into condieration. TD-Learning shifts the existing function according to the **TD target** given by $r_{t+1} + \\gamma V(s_{t+1})$ or $r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})$ respectively, which estimates the function in the observed trajectory:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{new function} &= (1 - \\alpha) \\cdot \\text{current function} + \\alpha \\cdot \\text{observed function} \\\\\n",
    "    \\text{new value} &= (1 - \\alpha) \\cdot \\text{old value} + \\alpha \\cdot \\text{TD target}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    V(s_t) &= (1 - \\alpha) V(s_t) + \\alpha (r_{t+1} + \\gamma V(s_{t+1})) \\\\\n",
    "    V(s_t) &= V(s_t) - \\alpha V(s_t) + \\alpha (r_{t+1} + \\gamma V(s_{t+1})) \\\\\n",
    "    V(s_t) &= V(s_t) + \\alpha (r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) &= (1 - \\alpha) Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})) \\\\\n",
    "    Q(s_t, a_t) &= Q(s_t, a_t) - \\alpha Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})) \\\\\n",
    "    Q(s_t, a_t) &= Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This process of updating a function by condidering the current estimate for that function is also called botstraping. It should also be noted, that TD-Learning can learn on past experiences and even incomplete trajectories as only a sequence $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$ needs to be considered. This may be of importance when considering RL problems which have no terminal state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy\n",
    "\n",
    "RL-Algorithms which learn from genereted trajectories face a dilemma. The generation of a trajectory $\\tau$ requires a policy $\\pi(a | s)$. On one hand, this policy should try to maximize the expected return so that the new trajectory can be used to further refine and exploit the learned parameters. On the other hand, this ploicy need to explore new options which have not been considered beforehand, else we might get stuck in a very suboptimal local extremum. \n",
    "\n",
    "Thus, we should take a brief look at the **$\\epsilon$-Greedy** policy. This policy contains a simple idea to balance exploration and exploration and explotation when generating a policy using the current learned model, using a hyperparameter $\\epsilon \\in (0, 1)$. When choosing an action $a$ in a given state $s$ the $\\epsilon$-Greedy policy will choose one of two options at random.\n",
    "\n",
    "The first option with propability $\\epsilon$ is to choose a random action $a$ from all available actions. This option contains the exploration aspect and hopefully lead to the discovery of new strategies.\n",
    "\n",
    "The second option with propability $1 - \\epsilon$ is to choose the action $a$ greedily based on the current learned information. This option contains the exploitation aspect, refining existing strategies further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction code examples\n",
    "To compare the different approaches, each method is applied to OpenAI Gym's Taxi environment. A taxi picks up a passenger from four possible locations and drops him off at the target destination. To achieve that goal the taxi can move in each of the four cardinal directions, as well as picking up and dropping off the passenger. The methods' efficiency was evaluated using the variable \"Timesteps\" which indicates the amount of actions taken by the taxi driver to reach the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: random action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1184\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env= gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "def print_frames(frames):\n",
    "    frame = frames[-1]\n",
    "    print(frame['frame'])\n",
    "    print(f\"Timestep: {len(frames)}\")\n",
    "\n",
    " \n",
    "print_frames(frames)\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "**SARSA** is the most basic TD-Learning algorithm for learning the action-value function $Q(s, a)$. The name reflects, that each learning step is done based on the sequence state-action-reward-state-action $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$. \n",
    "\n",
    "1. generate trajectory $\\tau$ using $\\epsilon$-Greedy policy.\n",
    "2. for each sequence $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$ adjust $Q(s_t, a_t)$ according to:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) &= Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "3. If learning is insufficient and time left go back to step 1. Else terminate the algorithm and output $Q(s, a)$ and greedy policy if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: SARSA learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 14\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v3').env\n",
    "\n",
    "# Hyperparameters\n",
    "q_table=np.zeros([env.observation_space.n, env.action_space.n])\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = []\n",
    "\n",
    "# Function to print out the game-state\n",
    "def print_frames(frames):\n",
    "    frame = frames[-1]\n",
    "    print(frame['frame'])\n",
    "    print(f\"Timestep: {len(frames)}\")\n",
    "\n",
    "          \n",
    "# Starting the SARSA learning\n",
    "for i in range(10000):\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    state = env.reset()\n",
    "    # Choosing action\n",
    "    if random.uniform(0, 1) < epsilon:  \n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    done=False\n",
    "  \n",
    "    while not done:\n",
    "          \n",
    "        # Getting the next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Choosing the next action\n",
    "        if random.uniform(0, 1) < epsilon:  \n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_action = np.argmax(q_table[next_state])\n",
    "  \n",
    "         \n",
    "        old_value = q_table[state, action]\n",
    "        new_value = q_table[next_state,next_action]        \n",
    "        #Learning the Q-value\n",
    "        q_table[state, action] = old_value + alpha * (reward + gamma * new_value - old_value)\n",
    "  \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "          \n",
    "        #Updating the respective vaLues\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        epochs+=1\n",
    "    \n",
    "    #Printing out episode counter\n",
    "    \n",
    "    #if i % 100 == 0:\n",
    "       #clear_output(wait=True)\n",
    "       #print(f\"Episode: {i}\")        \n",
    "            \n",
    "#Evaluating the performance\n",
    "done= False\n",
    "\n",
    "state=env.reset()\n",
    "\n",
    "while not done:\n",
    "        action=np.argmax(q_table[state])\n",
    "        state,reward,done,info=env.step(action)\n",
    "        \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward \n",
    "        }\n",
    "        )\n",
    "print_frames(frames)\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "**Q-Learning** is very similar to SARSA except for the utilization of a different TD-target. The new TD-target is $r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q(s_{t+1}, a)$. In effect, we no longer look at the reward returned in the observed trajectory but the maximum possible reward which can be obtained from state $s_{t+1}$. Thus we also no longer requre $a_{t+1}$ and a sequence now only contains $s_t, a_t, r_{t+1}, s_{t+1}$.\n",
    "\n",
    "1. generate trajectory $\\tau$ using $\\epsilon$-Greedy policy.\n",
    "2. for each sequence $s_t, a_t, r_{t+1}, s_{t+1}$ adjust $Q(s_t, a_t)$ according to:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) &= Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a \\in \\mathcal{A}} Q(s_{t+1}, a) - Q(s_t, a_t)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "3. If learning is insufficient and time left go back to step 1. Else terminate the algorithm and output $Q(s, a)$ and greedy policy if needed.\n",
    "\n",
    "**(maybe expand algorithms to not use trajectories but gradualy generated tuples)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 11\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env= gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "\n",
    "# Hyperparameters\n",
    "q_table=np.zeros([env.observation_space.n, env.action_space.n])\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = []\n",
    "\n",
    "\n",
    "\n",
    "# Function to print out the game-state\n",
    "def print_frames(frames):\n",
    "    frame = frames[-1]\n",
    "    print(frame['frame'])\n",
    "    print(f\"Timestep: {len(frames)}\")\n",
    "\n",
    "\n",
    "# Starting the Q-learning        \n",
    "for i in range(1, 10000):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choosing action\n",
    "        if random.uniform(0, 1) < epsilon:  \n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        # Getting next state\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        # learning the Q-value\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "# Evaluating the performance\n",
    "\n",
    "done= False\n",
    "\n",
    "state=env.reset()\n",
    "\n",
    "while not done:\n",
    "        action=np.argmax(q_table[state])\n",
    "        state,reward,done,info=env.step(action)\n",
    "        \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward \n",
    "        }\n",
    "        )\n",
    "print_frames(frames)\n",
    "        \n",
    "print(\"Training finished.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of examples\n",
    "By using one of the provided reinforcement algorithms the goal was reached in a much shorter timeframe than the control method. The two former performed similarly throughout the test period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On- vs Off-policy\n",
    "\n",
    "The change in the TD-target may seem small yet it is very important. SARSA estimates the Q-value assuming the $\\epsilon$-Greedy policy used to generate the data $s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$ continues to be followed. Thus, SARSA optimates the $\\epsilon$-Greedy policy and not the greedy policy. We call this **on-policy** since the policy used for data generation and updates are **the same**.\n",
    "\n",
    "In contrast, Q-Learning also generates data using the $\\epsilon$-Greedy policy, yet Q-Learning updates based on the greedy policy. Through this, Q-Learning always tries to improve the greedy policy. This behaviour is called **off-policy** since the policy used for data generation and updates are **not** the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Lilian Wang: A (Long Peek into Reinforcment Learning)](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)\n",
    "\n",
    "- [OpenAI: Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#optional-formalism)\n",
    "\n",
    "- [Reinforcement Learning: An introcution - Richard S. Sutton and Andred G. Barot](https://books.google.de/books?hl=de&lr=&id=uWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=richard+sutton+andrew+barto&ots=miqNm2-_i9&sig=Xv2GFGQyFAemej2n6HMvDU01oiE&redir_esc=y#v=onepage&q=richard%20sutton%20andrew%20barto&f=false)\n",
    "\n",
    "- [Q-Learning and SARSA, with Python](https://towardsdatascience.com/q-learning-and-sasar-with-python-3775f86bd178)\n",
    "\n",
    "- [Reinforcement Q-Learning from Scratch in Python with OpenAI Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
