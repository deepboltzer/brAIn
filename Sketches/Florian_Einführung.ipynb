{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104110dc",
   "metadata": {},
   "source": [
    "Introduction to RL\n",
    "\n",
    "Basic elements of RL: Having an \"Agent\" in some sort of \"Environment\". What is an agent? What is an\n",
    "environment? What is the \"goal\"?\n",
    "\n",
    "Agent: The \"Person\" operating in the given enviroment. Analyses the current situation around\n",
    "him(The result of this analysation is called \"State\") and takes a action according to the state.\n",
    "Different actions may be possible in a given state(see example)\n",
    "Example for human: \n",
    "State: Having a cup of coffee in his hand: \n",
    "Possible actions: Drink, put it down, ...?\n",
    "\n",
    "Environment: The surrounding of the agent. Analysing the situation the agent is in can be very\n",
    "simple or very complex: \n",
    "Example\n",
    "Easy: One romm with one chair: The Agent can either sit on the chair or stand next to it. Why is that Environment simple?    \n",
    "Complex: A Street full of moving cars: Why is that more complex? Alot more components that \n",
    "influence the state the Agent is in.(If the Agent is driving a car: A red light?)\n",
    "\n",
    "Question: How does the agent know which action to take? \n",
    "Solution: Reward!(Key Component of Reinforced Learning) What does reward mean and how does it work?:\n",
    "Whenever the agent takes an action he recieves a reward. These rewards may vary depending\n",
    "on the action the agent takes. \n",
    "\n",
    "\n",
    "EXAMPLE: \n",
    "State: Coffee in hand\n",
    "Actions: Drinking it +5(?) pouring it over yourself(-5?)\n",
    "Or another way to look at it: The state you land in after taking an action determines the reward:\n",
    "What does that mean? Whenever the Agent takes an Action he may think ahead of the consequences of\n",
    "said action. (Staying in example: Drinking coffee-> feeling awake(a good state=high reward)\n",
    "while Pouring it over yourself->dirty clothes(a bad state= bad reward)\n",
    "So just like a Human the Agent will take the Action with the most beneficial outcome.\n",
    "\n",
    "*INSERT ILLUSTRATION*\n",
    "//The given ILLUSTRATION gives a general overview//\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37402640",
   "metadata": {},
   "source": [
    "Takeaway: An Agent, just like a Human, is put into any Environment. He then has to decide\n",
    "between different Actions he can take. All these Actions are linked to a Reward, depending on\n",
    "outcome(state) the Agent ends in after taking said Action.\n",
    "\n",
    "But how does an Agent learn which Action to take in a specific state? How does he \n",
    "learn about his Environment to evaluate which Action to take in which state?\n",
    "These Questions bring us to our next Basic Element: The \"Model\" of an Environment.\n",
    "\n",
    "What happens in the Environment when you take a certain action? The answer is given to\n",
    "you by the Model of that Environment. \n",
    "Example: \n",
    "State: Agent holds Cup of Coffee\n",
    "Action: Releasing the Cup of Coffee \n",
    "Next State: <-- Determined by the Model!(So?)\n",
    "Possiblity 1: The Cup falls and breaks(What we would consider the normal Outcome)\n",
    "Possiblity 2: The Cup stays in Place\n",
    "Takeaway: Without knowing the Model its hard to predict what exactly will happen if a certain Action is taken.\n",
    "\n",
    "That leaves us with Two Scenarios:\n",
    "\n",
    "1.: We know every Aspect of the Model. \n",
    "2.: We do not know the Model and we have to \"learn\" a way to navigate in the Environment.\n",
    "\n",
    "For RL Case 1 is of no interest because you already know every possible outcome of an Action and there is nothign left for the Agent to learn but instead you simply tell him what to do in each situation.\n",
    "That brings us to Case 2. The Agent is put into an Environment without knowing its Model. The goal is\n",
    "for the Agent to learn a set of \"rules\" that allow him to always choose an Action in a state that will fetch\n",
    "him the biggest reward in the end.\n",
    "Example:\n",
    "Goal: Reaching your Home after Work in the shortest possible time\n",
    "Possible Actions: Walk, Bike, Car \n",
    "Solution: Even though the Car is faster in theory, due to traffic, taking the Bike is way faster -> The Agent takes the Bike, is home in the shortest possible time and fetches the highest possible Reward.\n",
    "These guidelines are called \" Policies\". It tells the Agent the optimal Action to take in a certain states to get the highest possible Reward in the end.\n",
    "\n",
    "Thinking about this Example you may ask yourself: Does the Agent really need to learn the Model or isn´t it enough to simply learn which Actions lead you to the biggest possible reward?\n",
    "\n",
    "This Question leads us to two Key Concepts of RL: Model-based and Model-free learning.\n",
    "Examples:\n",
    "Model-based: Supermarket: The Supermarket is closed on sunday. The agent can observe this state(closed on sunday)\n",
    "\n",
    "\n",
    "Model-free: Eating: Before Reserachers investigated why Humans have to eat they already knew the primal concept : \n",
    "State: Hungry\n",
    "Action 1: Eating -> State 1: Full/Survive(Reward +5)\n",
    "Action 2: Not Eating-> State 2: Starve/Die(Reward -5)\n",
    "Without understanding the Model behind the need to eat the Human learned that in order to survive(get a reward) he has to eat\n",
    "\n",
    "Takeaway: The key difference between model-based and model-free learning is the dependency on the model. While \n",
    "model-free simply sees a connection between a certain action and the reward and decides according to these informations,\n",
    "the model-based approach tries to understand and learn the exact model.\n",
    "\n",
    "That brings us back to our first question: How does the agent know which action to take?\n",
    "Let us rephrase that question a bit with our knewly gained knowledge:\n",
    "How can we evaluate the state we are about to enter if we take a certain action?\n",
    "We do that with a reward function R(s,a). It predicts the reward we get if we take\n",
    "an action a in step s. But simply looking at the next reward seems a bit naiv.\n",
    "Instead let´s try to predict not just the reward of the next action but also future \n",
    "rewards. \n",
    "\n",
    "Example:\n",
    "State: Money in the bank\n",
    "Possible action 1: leave money in the banks <-- more money later on\n",
    "Possible action 2: use money now\n",
    "\n",
    "Simply looking at these two actions without any future rewards would make the decision obvious:action 2 would give you a higher\n",
    "reward. But maybe in a few years you can buy yourself something that will return a much higher reward then what you would get right now. That concept is called the \"Value Function\"  $G_t$.\n",
    "\n",
    "Using this \"Value Function\" we can now determine two further functions we can use to decide which action to take.\n",
    "The \"state-value\" and the \"action-value\".\n",
    "\n",
    "state-value: $V(s) = \\textbf{E}\\left[ G_t | S_t = s \\right]$  gives you the expected return if you are in this sate at time t.\n",
    "\n",
    "and the action-value: $Q(s,a) = \\textbf{E}\\left[ G_t | S_t = s \\right]$ gives you the expected return if you are in state s and you would take action a.\n",
    "\n",
    "Both these functions are a sum of future rewards. If you are interested in the reward you would get in the next state + every future reward you will end up with a \"Bellman Equation\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
