{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc14142f",
   "metadata": {},
   "source": [
    "# Titel\n",
    "\n",
    "There is one reoccuring problem regarding policies in Reinforment Learning: How do you optimize\n",
    "a policy when you need to act non-optimally to explore all actions to find the optimal action?\n",
    "\n",
    "This problem leads to two core approaches regarding policy optimization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fad20f",
   "metadata": {},
   "source": [
    "## On-policy VS off-policy\n",
    "### On-policy\n",
    "The on-policy approach acts as a compromise by following an almost optimal policy with room for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ccff8",
   "metadata": {},
   "source": [
    "### Off-policy\n",
    "A different idea is using a separate policy (e.g. a greedy policy) to generate the data. This way the agent might choose actions not part of the target policy thus exploring a wider range of actions that will eventually be utilized to further optimize the target policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2829296",
   "metadata": {},
   "source": [
    "## Temporal-Difference Learning\n",
    "To get a better understanding of the two different concepts it's necessary to introduce the term Temporal-Difference Learning.TD Learning is a model-free method that learns from incomplete episodes. This means each episode doesnÂ´t have to terminate to be used to update the target policy. This is particular important because some episodes may take alot of time to terminate and thus  being able to learn from these episodes earlier can spare you alot of time. More specificly the TD methods only need to wait one additional time step to update the policy based on $R_{t+1}$ and estimate $V(S_{t+1})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9b519",
   "metadata": {},
   "source": [
    "### TD Learning on-policy example: SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd1a4c",
   "metadata": {},
   "source": [
    "### TD Learning off-policy example: Q-learning \n",
    "In Q-Learning, the agent learns optimal policy with the help of a greedy policy. It updates its Q-Values according to this equation:\n",
    "\\begin{align}\n",
    "Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha \\left[R_{t+1}+\\gamma \\max_{{a}} Q(S_{t+1},a)-Q(S_t,A_t)\\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To get a better understanding of the efficiency of Q-learning an example is provided. It compares a solution using dynamic programming to one using Q-learning. The problem consists of a taxi that starts at a random location and has to pick up a customer and drop him off at a target location. The pick up and drop off locations are chosen randomly from 4 different possibilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b19fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 502\n",
      "State: 0\n",
      "Action: 5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env= gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "def print_frames(frames):\n",
    "    frame = frames[-1]\n",
    "    print(frame['frame'])\n",
    "    print(f\"Timestep: {len(frames)}\")\n",
    "    print(f\"State: {frame['state']}\")\n",
    "    print(f\"Action: {frame['action']}\")\n",
    " \n",
    "print_frames(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b98ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 14\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env= gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "\n",
    "# Hyperparameters\n",
    "q_table=np.zeros([env.observation_space.n, env.action_space.n])\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = []\n",
    "\n",
    "\n",
    "def print_frames(frames):\n",
    "    frame = frames[-1]\n",
    "    print(frame['frame'])\n",
    "    print(f\"Timestep: {len(frames)}\")\n",
    "    print(f\"State: {frame['state']}\")\n",
    "    print(f\"Action: {frame['action']}\")\n",
    "    print(f\"Reward: {frame['reward']}\")\n",
    "        \n",
    "for i in range(1, 10000):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "   # if i % 100 == 0:\n",
    "      #  clear_output(wait=True)\n",
    "       # print(f\"Episode: {i}\")\n",
    "\n",
    "done= False\n",
    "\n",
    "state=env.reset()\n",
    "\n",
    "while not done:\n",
    "        action=np.argmax(q_table[state])\n",
    "        state,reward,done,info=env.step(action)\n",
    "        \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward \n",
    "        }\n",
    "        )\n",
    "print_frames(frames)\n",
    "        \n",
    "print(\"Training finished.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399039af",
   "metadata": {},
   "source": [
    "The difference in timesteps taken for the two alogirthms to finish the task gives a good understanding about the amount of time saved by using a TD learning based algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889baf5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Reinforment Learning: An introcution - Richard S. Sutton and Andred G. Barot](https://books.google.de/books?hl=de&lr=&id=uWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=richard+sutton+andrew+barto&ots=miqNm2-_i9&sig=Xv2GFGQyFAemej2n6HMvDU01oiE&redir_esc=y#v=onepage&q=richard%20sutton%20andrew%20barto&f=false)\n",
    "- [Lilian Wang: A (Long Peek into Reinforcement Learning)](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51365173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
