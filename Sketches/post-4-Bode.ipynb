{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Policy Gradients\n",
    "\n",
    "This post will introduce the core concepts underlying various **policy gradient algorithms**. These algorithms are an approach to reinforcement learning utilizing **stochastic gradient ascent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "Those of you familiar with neural networks will probably have heard of stochastic gradient descent. The goal of stochastic gradient descent is to calculate the gradient of a loss-function to then adjust the parameters of the network to minimize the loss-function by steping in the opposite direction of the gradient.\n",
    "\n",
    "We can utlize the very similar stochastic gradient ascent to appraoch reinforcment learning problems. To do this, we will need a reward function $J(\\theta)$ which tells us how well a given policy $\\pi_\\theta$ performs using the parameters $\\theta$.  In reinforcment learning this is actually quite simple since reinforcment inherintly utilizes rewards $r$. Thus the reward function is chosen to be the exoected reward for a trajectory $\\tau$ generated by the current policy $\\pi_\\theta$. Let $G(\\tau)$ be the infinite horizon discounted-return starting at timestep $t = 0$ for the trajectory $\\tau$ (the derivations of finite or undiscounted return are almost identical). \n",
    "\n",
    "\\begin{align}\n",
    "    G(\\tau) &= \\sum_{t = 1}^\\infty \\gamma^t r_{t+1} \\\\\n",
    "    J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta} [ G(\\tau) ] \\\\\n",
    "\\end{align}\n",
    "\n",
    "If it is possible to now calculate the gradient of the reward function $J(\\theta)$ we can take a stochastic gradient ascent step based on a hyperparameter $\\alpha$ to slowly over many steps maximise the reward function:\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta = \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "The only remaining probelem is to actually determine how the gradient $\\nabla_\\theta J(\\theta)$ looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the Gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal\n",
    "\n",
    "why use the gradient (intuition)\n",
    "\n",
    "equation to maximise.\n",
    "\n",
    "given a policy, form of one step\n",
    "\n",
    "\n",
    "to add: curse of dimensionality, previous approaches need to analyze entire action space\n",
    "\n",
    "## how to calculate the gradient\n",
    "\n",
    "derive simple form of policy gradient (see open AI spinning up)\n",
    "\n",
    "## other forms of the policy gradient\n",
    "\n",
    "list the other typical forms of policy gradients \n",
    "\n",
    "policy gradient theorem\n",
    "\n",
    "## simple policy gradient algorithm\n",
    "\n",
    "reinforce or something similarly basic\n",
    "\n",
    "## more advanced concepts?\n",
    "\n",
    "baselines\n",
    "\n",
    "TRPO\n",
    "\n",
    "PPO\n",
    "\n",
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
