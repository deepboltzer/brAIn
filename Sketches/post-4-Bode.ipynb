{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Policy Gradients\n",
    "\n",
    "This post will introduce the core concepts underlying various **policy gradient algorithms**. These algorithms are an approach to reinforcement learning utilizing **stochastic gradient ascent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "Those of you familiar with neural networks will probably have heard of stochastic gradient descent. The goal of stochastic gradient descent is to calculate the gradient of a loss-function to then adjust the parameters of the network to minimize the loss-function by steping in the opposite direction of the gradient.\n",
    "\n",
    "We can utlize the very similar stochastic gradient ascent to appraoch reinforcment learning problems. To do this, we will need a reward function $J(\\theta)$ which tells us how well a given policy $\\pi_\\theta$ performs using the parameters $\\theta$.  In reinforcment learning this is actually quite simple since reinforcment inherintly utilizes rewards $r$. Thus the reward function is chosen to be the exoected reward for a trajectory $\\tau$ generated by the current policy $\\pi_\\theta$. Let $G(\\tau)$ be the infinite horizon discounted-return starting at timestep $t = 0$ for the trajectory $\\tau$. \n",
    "\n",
    "The derivation of the policy gradient using finite or undiscounted return are almost identical. For finite horizon $T$ simply replace all $\\infty$ with $T$.\n",
    "\n",
    "\\begin{align}\n",
    "    G(\\tau) &= \\sum_{t = 1}^\\infty \\gamma^t r_{t+1} \\\\\n",
    "    J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta} [ G(\\tau) ] \\\\\n",
    "\\end{align}\n",
    "\n",
    "If it is possible to now calculate the gradient of the reward function $J(\\theta)$ we can take a stochastic gradient ascent step based on a hyperparameter $\\alpha$ to slowly over many steps maximise the reward function:\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta = \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "The only remaining probelem is to actually determine how the gradient $\\nabla_\\theta J(\\theta)$ looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the Gradient\n",
    "\n",
    "##### Step 1:\n",
    "\n",
    "We now that if $X$ is a random variable, $P(x)$ is the probability that $X = x$ and we want to calculate $\\mathbb{E}[X]$ we can do this by calculating $\\mathbb{E}[X] = \\sum_x P(x) x$ and thereby summing over all possible values of $X=x$ multiplied with their respective probability.\n",
    "\n",
    "We can do something similar for $J(\\theta)$ by summing over all posible trajectories $\\tau$ if we rely on the probability that $\\tau$ occurs given policy $\\pi_\\theta$ determined by paramteres $\\theta$ $P(\\tau | \\theta)$.\n",
    "\n",
    "\\begin{align}\n",
    "    J(\\theta) &= \\sum_\\tau P(\\tau | \\theta) G(\\tau) \\\\\n",
    "    \\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta} [ G(\\tau) ] \\\\\n",
    "    &= \\nabla_\\theta \\sum_\\tau P(\\tau | \\theta) G(\\tau) \\\\\n",
    "    &= \\sum_\\tau \\nabla_\\theta P(\\tau | \\theta) G(\\tau) \\\\\n",
    "\\end{align}\n",
    "\n",
    "##### Step 2:\n",
    "\n",
    "Given a function $f(x)$ the log-derivative trick is useful for rewriting the derivative $\\frac{d}{dx} f(x)$ and relies upon the derivative of $\\log(x)$ being $\\frac{1}{x}$ and the chain rule.\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d}{dx} f(x) &= f(x) \\frac{1}{f(x)} \\frac{d}{dx} f(x) \\\\\n",
    "    &= f(x)\\frac{d}{dx} \\log(f(x)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since $G(\\tau)$ is not dependant on $\\theta$ we can apply this trick to $P(\\tau | \\theta)$:\n",
    "\n",
    "\\begin{align}\n",
    "    J(\\theta) &= \\sum_\\tau \\nabla_\\theta P(\\tau | \\theta) G(\\tau) \\\\\n",
    "    &= \\sum_\\tau P(\\tau | \\theta) \\nabla_\\theta \\log(P(\\tau | \\theta)) G(\\tau) \\\\\n",
    "\\end{align}\n",
    "\n",
    "##### Step 3:\n",
    "\n",
    "An equation for $P(\\tau | \\theta)$ is still required, but can be found by considering the problem as an markov decision process (MDP). Let $p(s_1)$ be the probability of starting a trajectory in state $s_1$, then $P(\\tau | \\theta)$ can be expressed by multiplying the probabilities for each occurence of all states $s_t$ and actions $a_t$.\n",
    "\n",
    "\\begin{align}\n",
    "    P(\\tau | \\theta) &= p(s_1) \\prod_{t = 1}^\\infty P(s_{t+1} | s_t, a_t) \\pi_\\theta(a_t | s_t) \\\\\n",
    "    \\log(P(\\tau | \\theta)) &= \\log(p(s_1)) \\sum_{t = 1}^\\infty \\big( \\log(P(s_{t+1} | s_t, a_t)) + \\log(\\pi_\\theta(a_t | s_t)\\big)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "##### Step 4\n",
    "\n",
    "Notice that $p(s_1)$ and $P(s_{t+1} | s_t, a_t)$ are also not dependant on $\\theta$. \n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_\\theta \\log(P(\\tau | \\theta)) &= \\sum_{t = 1}^\\infty \\pi_\\theta(a_t | s_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_\\theta J(\\theta) &= \\sum_\\tau P(\\tau | \\theta) \\sum_{t = 1}^\\infty \\log(\\pi_\\theta(a_t | s_t)) G(\\tau) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now we can reverse step 1 leaving us with finished equation which can be estimated by sampling multiple trajectories and taking the mean of the gradients for the sampled trajcetories.\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_\\theta J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t = 1}^\\infty \\log(\\pi_\\theta(a_t | s_t)) G(\\tau) \\right] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal\n",
    "\n",
    "why use the gradient (intuition)\n",
    "\n",
    "equation to maximise.\n",
    "\n",
    "given a policy, form of one step\n",
    "\n",
    "\n",
    "to add: curse of dimensionality, previous approaches need to analyze entire action space\n",
    "\n",
    "## how to calculate the gradient\n",
    "\n",
    "derive simple form of policy gradient (see open AI spinning up)\n",
    "\n",
    "## other forms of the policy gradient\n",
    "\n",
    "list the other typical forms of policy gradients \n",
    "\n",
    "policy gradient theorem\n",
    "\n",
    "## simple policy gradient algorithm\n",
    "\n",
    "reinforce or something similarly basic\n",
    "\n",
    "## more advanced concepts?\n",
    "\n",
    "baselines\n",
    "\n",
    "TRPO\n",
    "\n",
    "PPO\n",
    "\n",
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
